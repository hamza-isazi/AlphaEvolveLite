db_uri: sqlite:///alphaevolve.db

experiment:
  label: canterbury-compression-openrouter-ensemble
  notes: "Canterbury corpus compression experiment with no imports and tabu search inspired prompting"
  save_top_k: 5               # number of top programs to save one completion of the experiment

llm:
  provider: openrouter
  llm_timeout: 300.0  # Global timeout for all LLM requests
  models:
    # OpenAI models - high performance, reliable
    # - name: openai/gpt-4.1
    #   probability: 0.05
    #   temperature: 0.7
    # - name: openai/o4-mini-high
    #   probability: 0.05
    #   retry_model: google/gemini-2.5-flash
    - name: openai/gpt-4.1-mini
      probability: 0.05
      temperature: 0.7
    # GPT-4o-mini is replaced by GPT-OSS-120B
    # - name: openai/gpt-4o-mini
    #   probability: 0.1
    #   temperature: 0.7
    - name: openai/gpt-oss-120b
      probability: 0.1
      temperature: 0.7
    
    # Google Gemini models - strong reasoning capabilities, decent cost
    - name: google/gemini-2.5-pro
      probability: 0.25
      temperature: 0.7
      retry_model: google/gemini-2.5-flash
    - name: google/gemini-2.5-flash
      probability: 0.1
      temperature: 0.7
      reasoning_effort: medium
    - name: google/gemini-2.5-flash-lite
      probability: 0.05
      temperature: 0.7
      reasoning_effort: medium
    
    # Anthropic Claude models - excellent for complex reasoning, high cost
    # Opus is a bit too expensive and not very accurate for this experiment, replaced by Grok4
    # - name: anthropic/claude-opus-4
    #   probability: 0.025
    #   temperature: 0.7
    
    - name: anthropic/claude-sonnet-4
      probability: 0.05
      temperature: 0.7
      retry_model: google/gemini-2.5-flash

    # - name: anthropic/claude-3.5-haiku
    #   probability: 0.05
    #   temperature: 0.7
    #   retry_model: google/gemini-2.5-flash

    # DeepSeek models - strong reasoning capabilities, free, and should be quite diverse
    - name: deepseek/deepseek-chat-v3-0324:free
      probability: 0.05
      temperature: 0.7

    # - name: deepseek/deepseek-r1:free
    #   probability: 0.05
    #   temperature: 0.7

    # Qwen3 is one of the best models for code generation and is quite cheap
    - name: qwen/qwen3-coder
      probability: 0.1
      temperature: 0.7

    # Codestral is a good model for code generation and is quite cheap
    - name: mistralai/codestral-2508
      probability: 0.05
      temperature: 0.7
      retry_model: google/gemini-2.5-flash

    # Maverick is high capacity and quite cheap because of MoE
    - name: meta-llama/llama-4-maverick
      probability: 0.05
      temperature: 0.7
      retry_model: google/gemini-2.5-flash

    # # Horizon, is great for code generation, but unavailable atm
    # - name: openrouter/horizon-beta
    #   probability: 0.1
    #   temperature: 0.7
    #   retry_model: google/gemini-2.5-flash

    # Grok4 is supposed to be one of the most powerful models out there, but quite expensive
    - name: x-ai/grok-4
      probability: 0.05
      temperature: 0.7
      retry_model: google/gemini-2.5-flash

    # Kimi-K2 is a good model for code generation and free
    - name: moonshotai/kimi-k2:free
      probability: 0.05
      temperature: 0.7
      retry_model: google/gemini-2.5-flash

    # GLM-4.5 is a large model and quite cheap
    - name: z-ai/glm-4.5
      probability: 0.05
      temperature: 0.7
      retry_model: google/gemini-2.5-flash
    
  system_prompt: |
    You are tasked with implementing a Python text compression algorithm.
    Your program must define two functions:

        def compress(text: str) -> bytes
        def decompress(data: bytes) -> str

    The goal is to minimize the size of the compressed data while ensuring perfect reversibility. Your solution will be evaluated on the Canterbury Corpus, a benchmark dataset containing a variety of file types (e.g., English text, code, spreadsheets).

    Constraints:
    - Your solution must not use any external libraries.
    - The `compress()` and `decompress()` functions must be defined in a single file called `solution.py`.
    - Compression must be lossless.
    - You may only use imports from the Python standard library (no third-party packages).
    - You may NOT use any compression-related libraries from the standard library, including: zlib, bz2, lzma, gzip, zipfile, tarfile, or any other compression modules.

    Your program will be evaluated using a score of `1.0 / total_compressed_size_in_bytes`, summed across all corpus files. Smaller total size = higher score.

evolution:
  population_size: 20
  temperature: 1.5
  max_generations: 100
  inspiration_count: 2        # number of inspirations to use in each generation
  max_retries: 5              # number of retries for failed program generation
  eval_timeout: 1800.0         # timeout in seconds for evaluation runs (0 = no timeout)
  enable_feedback: false      # Enable LLM-generated feedback for successful programs
  selection_method: enhanced_inspiration  # Method: "boltzmann", "top_k_and_random", or "enhanced_inspiration"
  recent_generations: 5       # Number of recent generations to consider for inspiration selection
  recent_percentile: 10.0     # Percentile threshold for recent generation selection (0-100)
  tabu_search_probability: 0.1  # Probability of using tabu search (fundamentally new approach) vs improvement


problem:
  entry_script: examples/canterbury_compression/initial_program.py
  evaluator:    examples/canterbury_compression/evaluate.py
